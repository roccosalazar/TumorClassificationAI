import numpy as np
import unittest

class MetricsCalculator:
    def __init__(self, input_data, method="holdout"):
        """
        Inizializza l'input e il metodo di validazione.

        Args:
            input_data: Una matrice contenente gli array numpy di y_real e y_pred.
            method (str): Metodo di validazione ("holdout" o "Leave-p-out Cross Validation").
        """
        self.input_data = input_data
        self.method = method

    def accuracy(self, y_real, y_pred):
        """
        Calcola il tasso di accuratezza.
        """
        return np.mean(y_real == y_pred)

    def error_rate(self, y_real, y_pred):
        """
        Calcola il tasso di errore.
        """
        return 1 - self.accuracy(y_real, y_pred)

    def sensitivity(self, y_real, y_pred):
        """
        Calcola la sensibilità (recall per la classe positiva).
        """
        TP = np.sum((y_real == 1) & (y_pred == 1))
        FN = np.sum((y_real == 1) & (y_pred == 0))
        return TP / (TP + FN) if (TP + FN) > 0 else 0

    def _calculate_metrics(self, y_real, y_pred):
        """
        Calcola le metriche per una singola coppia di array.
        """
        return {
            "Accuracy": self.accuracy(y_real, y_pred),
            "Error Rate": self.error_rate(y_real, y_pred),
            "Sensitivity": self.sensitivity(y_real, y_pred)
        }

    def calculate_metrics(self):
        """
        Calcola le metriche in base al metodo di validazione.
        """
        if self.method == "holdout":
            # Singola coppia di array
            y_real, y_pred = self.input_data[0]
            return self._calculate_metrics(y_real, y_pred)

        elif self.method == "Leave-p-out Cross Validation":
            # Più coppie di array
            metrics_list = []
            total_tp = 0
            total_fn = 0

            for y_real, y_pred in self.input_data:
                metrics = self._calculate_metrics(y_real, y_pred)
                metrics_list.append(metrics)

                # Accumula TP e FN per calcolo globale della Sensitivity
                total_tp += np.sum((y_real == 1) & (y_pred == 1))
                total_fn += np.sum((y_real == 1) & (y_pred == 0))

            # Aggrega le metriche
            aggregated_metrics = {
                metric: np.mean([m[metric] for m in metrics_list])
                for metric in metrics_list[0]
            }

            # Calcola la Sensitivity aggregata
            aggregated_metrics["Sensitivity"] = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0

            return aggregated_metrics

        else:
            raise ValueError("Metodo non supportato. Usa 'holdout' o 'Leave-p-out Cross Validation'.")

class TestMetricsCalculator(unittest.TestCase):
    def test_holdout_accuracy(self):
        y_real = np.array([1, 0, 1, 1, 0])
        y_pred = np.array([1, 0, 1, 0, 0])
        input_data = [(y_real, y_pred)]
        calculator = MetricsCalculator(input_data, method="holdout")
        metrics = calculator.calculate_metrics()
        self.assertAlmostEqual(metrics["Accuracy"], 0.8)

    def test_holdout_error_rate(self):
        y_real = np.array([1, 0, 1, 1, 0])
        y_pred = np.array([1, 0, 1, 0, 0])
        input_data = [(y_real, y_pred)]
        calculator = MetricsCalculator(input_data, method="holdout")
        metrics = calculator.calculate_metrics()
        self.assertAlmostEqual(metrics["Error Rate"], 0.2)

    def test_holdout_sensitivity(self):
        y_real = np.array([1, 0, 1, 1, 0])
        y_pred = np.array([1, 0, 1, 0, 0])
        input_data = [(y_real, y_pred)]
        calculator = MetricsCalculator(input_data, method="holdout")
        metrics = calculator.calculate_metrics()
        self.assertAlmostEqual(metrics["Sensitivity"], 0.6666666666666666)

    def test_LeavePoutCrossValidation_aggregation(self):
        input_data = [
            (np.array([1, 0, 1]), np.array([1, 0, 1])),
            (np.array([1, 1, 0]), np.array([1, 0, 0]))
        ]
        calculator = MetricsCalculator(input_data, method="kfold")
        metrics = calculator.calculate_metrics()
        self.assertAlmostEqual(metrics["Accuracy"], 0.8333333333333333)
        self.assertAlmostEqual(metrics["Error Rate"], 0.16666666666666669)
        self.assertAlmostEqual(metrics["Sensitivity"], 0.75)

if __name__ == "__main__":
    unittest.main()
